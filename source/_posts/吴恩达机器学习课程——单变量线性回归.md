---
title: 吴恩达机器学习课程——单变量线性回归
comments: true
date: 2022-01-11 16:48:32
categories:
tags:
---

## 线性回归

在上一篇文章中，我们介绍了机器学习可以划分为监督学习（Supervised Learning）与无监督学习（Unsupervised Learning），监督学习即一致数据的正确答案：'right answer' for each example in the data。

回归是指我们预测出一个具体的数值输出。

在线性回归中，有以下几个基本概念：

>    training set 训练数据集
>
> ​	m = 训练样本的数量
>
> ​	x = 输入变量/特征
>
> ​	y = 输出变量/（预测的）目标变量
>
> ​	(x, y) = 一个训练样本
>
> ​	(x^(i), y^(i)) = 特定的训练样本，第i个训练样本（i是上标）
>
> ​	h = 假设函数，h maps from x's to y's

监督学习算法工作一般流程：向学习算法提供训练数据集合，学习算法的任务是输出一个函数，通常以h表示，h表示假设函数，假设函数的作用是作用于新数据，从而得出预测的结果，如下图所示。

![image-20220111175047682](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220111175047682.png)

当我们设计一个“学习算法”时，一个需要做的事是，决定怎么表示这个假设函数`h`。若预测的函数h可表示为如下形式：

> h(x) = ax + b

则这就是一个一元线性回归，或单变量线性回归。



## 代价函数

代价函数解决的问题是，如何把最有可能的直线与我们的数据相拟合。

在上面的公式中，a, b被称为模型参数，我们要做的就是谈谈如何选择这两个参数值，要尽量选择参数值，使得在训练集中，给出训练集中的x值，我们能合理准确地预测y的值。用数学语言表达如下：

>  标准定义：在线性回归中，我们要解决的是一个最小化问题：
>
> 目标函数：min<sub>a, b</sub> = (h(x) - y)<sup>2</sup> 

进一步，某个训练样本`i`：(![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps1.jpg))表示第*i*个样本, m表示训练样本总数目。

目标函数：

![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps2.jpg)

进一步：**整体目标函数**为：

![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps3.jpg)

其中，![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps4.jpg)为假设函数。表述为**找到**![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps6.jpg)**使得上述表达式的值最小**。

定义一个**代价函数**：（有时也称为平方误差函数，平方误差代价函数）


![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps7.jpg) 

需要做的就是关于![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps10.jpg)对函数![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps12.jpg)求最小值。

也存在其他的代价函数，但平方误差函数式解决问题最常用的手段。



