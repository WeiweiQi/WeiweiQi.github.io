---
title: 吴恩达机器学习——代价函数与梯度下降
comments: true
date: 2022-01-12 10:01:22
categories:
tags:
---





## 续上篇

在上一篇文章《[吴恩达机器学习课程——单变量线性回归](https://bbs.huaweicloud.com/blogs/325334)》中，我们了解了线性回归的基本概念以及代价函数的数学表达式。今天继续来研究一下这个数学函数。

![img](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/wps7.jpg)

## 代价函数的图像

在线性回归中，我们假设其函数为:h(x) = ax + b，我们假设b = 0，则假设函数为：

>  h(x) = ax

代价函数为：

![image-20220112162923346](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112162923346.png)

对于固定的测试数据集合，其中的m, x(1...m)，y(1...m)都是固定的，也就是说，J(a)是一个以a为自变量，J(a)值为因变量的函数，自然，我们就可以画出其函数图像。

若只有一个自变量a，则可以使用平面直角坐标系表示(a, J(a))的图像关系，通常情况下，其图像如下所示：

![image-20220112163127238](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112163127238.png)

上图左侧是h(x)的图像，右侧是对应每一个可能的取值a，即图中的θ<sub>1</sub>，J（θ<sub>1</sub>）的取值图像。

当不忽略b的取值时，代价函数为J(θ<sub>0</sub>，θ<sub>1</sub>)，其图像很有可能是类似下图的3D曲面图与等高线图：

![image-20220112164003170](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112164003170.png)



## 梯度下降： Gradient descent

梯度下降：一种可以自动找到使代价函数J(θ<sub>0</sub>，θ<sub>1</sub>)最小的θ<sub>0</sub>，θ<sub>1</sub>的算法。

其思路是：

1. start with some θ<sub>0</sub>，θ<sub>1</sub>, 以某些取值开始，通常令θ<sub>0</sub> = 0，θ<sub>1</sub> = 0 ；
2. 不断的改变θ<sub>0</sub>，θ<sub>1</sub>的值，降低J(θ<sub>0</sub>，θ<sub>1</sub>)
3. 直到找到一个最小值（可能是全局最小值，也可能是局部最小值）

第一次选择的起始点不同，得到的局部最小值可能不同。

其搜索路径如下图：

![image-20220112164518256](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112164518256.png)

梯度下降算法的定义：

![image-20220112165727419](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112165727419.png)

其中，

> convergence: 收敛
> “:=” 表示赋值；
> α表示学习率(learning rate)，用来控制梯度下降时我们迈出多大的步子，如果α很大，梯度下降就很迅速，我们会用大步子下山；如果α很小，那么我们会迈着很小的小碎步下山。
>
> ![image-20220112172253401](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112172253401.png) 表示其导数项。

对这个方程，你需要**同步更新(simultaneously update)(θ<sub>0</sub>，θ<sub>1</sub>)。吴恩达老师特意强调了，在更新θ<sub>0</sub>，θ<sub>1</sub>时，必须同步更新，以下更新方式是错误的：

![image-20220112172108297](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112172108297.png)

无论导数正负，都可以将变化趋势向最低点靠近：

![image-20220112173100466](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112173100466.png)

不同的α值的搜索效果如下图所示：如果太小会造成搜索过慢，如果太大则有可能不收敛失败。

![image-20220112173150810](https://gitee.com/wieweicoding/kevinqimgs/raw/master/img/image-20220112173150810.png)









